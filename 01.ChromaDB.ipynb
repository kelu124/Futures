{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, openai\n",
    "import time\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "import string\n",
    "LETTERS = string.ascii_uppercase\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31832/1528002886.py:11: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  underlying_embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OAI\"))\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
    "    openai.api_key = os.getenv(\"OAI\")\n",
    "    print (\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print (\"OPENAI_API_KEY environment variable not found\")\n",
    "\n",
    "underlying_embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OAI\"))\n",
    "\n",
    "\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings, store, namespace=underlying_embeddings.model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is ready\n",
      "2039 articles of good lengths in the articles.partquet.gzip\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"OAI\") is not None:\n",
    "    openai.api_key = os.getenv(\"OAI\")\n",
    "    print (\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print (\"OPENAI_API_KEY environment variable not found\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OAI\"))\n",
    "\n",
    "df = pd.read_parquet('data/articles.parquet.gzip')\n",
    "df.columns = [\"src\",\"content\",\"LEN\"]\n",
    "df = df[(df.LEN > 1500) & (df.LEN < 30000)].reset_index(drop=True)\n",
    "print(len(df),\"articles of good lengths in the articles.partquet.gzip\")\n",
    "titles = pd.read_parquet(\"data/titles.parquet.gzip\")\n",
    "df = df.merge(titles, on=\"src\",how=\"left\")\n",
    "mt = pd.read_parquet(\"data/metatags.parquet.gzip\")\n",
    "df = df.merge(mt,on=\"src\",how=\"left\")\n",
    "df.to_parquet(\"data/consolidated.parquet.gzip\",compression=\"gzip\")\n",
    "#df[\"text\"] = df.content\n",
    "df[\"source\"] = df.url\n",
    "df[\"author\"] = df.origin.apply(lambda x: \"kelu\" if str(x).startswith(\"20\") else \"other\")\n",
    "for x in df.columns:\n",
    "    df[x] = df[x].astype(str)\n",
    "df_loader = DataFrameLoader(df, page_content_column=\"content\")\n",
    "\n",
    "df_document = df_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2039 docs to add\n",
      "OPENAI_API_KEY is ready\n",
      "Continue on the DB\n",
      "2031 elements already stored.\n",
      "Already 2031 documents.\n",
      "Adding 2039 documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_splitter = CharacterTextSplitter(separator='\\n\\n',chunk_size=2000, chunk_overlap=200)\n",
    "chunked_documents = text_splitter.split_documents(df_document)\n",
    "print(len(chunked_documents),\"docs to add\")\n",
    "\n",
    "base_path = \"./DB/\"\n",
    "if os.getenv(\"OAI\") is not None:\n",
    "    openai.api_key = os.getenv(\"OAI\")\n",
    "    print (\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print (\"OPENAI_API_KEY environment variable not found\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OAI\"))\n",
    "\n",
    "\n",
    "if not os.path.isfile(base_path+\"chroma.sqlite3\"):\n",
    "    print(\"Start a new DB\")\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=[chunked_documents[0]],\n",
    "        embedding=embeddings,\n",
    "        persist_directory=base_path\n",
    "    )\n",
    "    vectordb.persist()\n",
    "else:\n",
    "    print(\"Continue on the DB\")\n",
    "    vectordb = Chroma(persist_directory=base_path,embedding_function=embeddings)\n",
    "    print(len(vectordb.get()[\"ids\"]),\"elements already stored.\")\n",
    "    LSDOCS = vectordb.get()[\"documents\"]\n",
    "\n",
    "print(\"Already\",len(vectordb.get()[\"documents\"]),\"documents.\")\n",
    "print(\"Adding\",len(chunked_documents),\"documents.\")\n",
    "\n",
    "if vectordb:\n",
    "    LSDOCS = vectordb.get()[\"documents\"]\n",
    "else:\n",
    "    LSDOCS = []\n",
    "    \n",
    "for doc in chunked_documents:\n",
    "    # Check if the text already exists somewhere\n",
    "    if not doc.page_content in LSDOCS:\n",
    "        vectordb.add_documents(\n",
    "            documents=[doc], \n",
    "            embedding=embeddings, \n",
    "            persist_directory=base_path\n",
    "        )\n",
    "        # Ugly hack to avoid reaching token per min limit \n",
    "        # So it sleeps 1s between page\n",
    "        time.sleep(0.001)\n",
    "        vectordb.persist()\n",
    "    else:\n",
    "        0\n",
    "        #print(\"Item already in the DB\",doc.page_content[:100].replace(\"\\n\",\" \"))\n",
    "LSDOCS = vectordb.get()[\"documents\"]\n",
    "vectordb.persist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
