# __Nvidia's AI Chips Outpace Moore's Law, According to CEO Jensen Huang__, from ([4628.0](https://kghosh.substack.com/p/4628.0).)

__[External link](https://techcrunch.com/2025/01/07/nvidia-ceo-says-his-ai-chips-are-improving-faster-than-moores-law/)__



## Keywords

* N
* v
* i
* d
* i
* a
* ,
*  
* A
* I
* ,
*  
* J
* e
* n
* s
* e
* n
*  
* H
* u
* a
* n
* g
* ,
*  
* M
* o
* o
* r
* e
* '
* s
*  
* L
* a
* w
* ,
*  
* t
* e
* c
* h
* n
* o
* l
* o
* g
* y
* ,
*  
* c
* h
* i
* p
* s
* ,
*  
* i
* n
* f
* e
* r
* e
* n
* c
* e
* ,
*  
* p
* e
* r
* f
* o
* r
* m
* a
* n
* c
* e
* ,
*  
* d
* a
* t
* a
*  
* c
* e
* n
* t
* e
* r

## Themes

* N
* v
* i
* d
* i
* a
* ,
*  
* A
* I
*  
* c
* h
* i
* p
* s
* ,
*  
* M
* o
* o
* r
* e
* '
* s
*  
* L
* a
* w
* ,
*  
* t
* e
* c
* h
* n
* o
* l
* o
* g
* y
*  
* a
* d
* v
* a
* n
* c
* e
* s
* ,
*  
* J
* e
* n
* s
* e
* n
*  
* H
* u
* a
* n
* g
* ,
*  
* d
* a
* t
* a
*  
* c
* e
* n
* t
* e
* r
* ,
*  
* A
* I
*  
* w
* o
* r
* k
* l
* o
* a
* d
* ,
*  
* i
* n
* f
* e
* r
* e
* n
* c
* e
* ,
*  
* c
* o
* s
* t
*  
* o
* f
*  
* c
* o
* m
* p
* u
* t
* i
* n
* g

## Other

* Category: technology
* Type: news

## Summary

Nvidia CEO Jensen Huang asserts that the performance of Nvidia's AI chips is evolving faster than Moore's Law, which predicts that the number of transistors on chips doubles approximately every two years. During a keynote at CES, Huang highlighted that Nvidia's latest superchip is over 30 times faster for AI inference workloads compared to its predecessor. He emphasized the simultaneous innovation across architecture, chips, systems, libraries, and algorithms as a key factor for this accelerated progress. Huang refutes claims of a slowdown in AI advancements, introducing three active AI scaling laws: pre-training, post-training, and test-time compute. He predicts that increasing computing capability will lower inference costs over time, with Nvidia's chips reportedly being 1,000 times better than those from a decade ago, suggesting a sustained trend of rapid improvement in AI technology.

## Signals

| name                                    | description                                                                                        | change                                                                                             | 10-year                                                                                                        | driving-force                                                                                            |   relevancy |
|:----------------------------------------|:---------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------|------------:|
| AI Chips Advancing Beyond Moore's Law   | Nvidia's AI chips are reportedly advancing faster than the historical rates of Moore's Law.        | The transition from traditional chip performance improvements to accelerated AI chip developments. | In 10 years, AI chips may be the primary drivers of computing innovation and cost reduction.                   | The need for higher performance in AI applications is pushing chip development beyond historical limits. |           4 |
| Emergence of New AI Scaling Laws        | Huang introduces three active AI scaling laws: pre-training, post-training, and test-time compute. | Shifting from reliance on Moore's Law to new scaling methods for AI model training.                | New scaling laws may redefine how AI models are developed and optimized over the next decade.                  | The complexity of AI models requires innovative approaches beyond traditional computing paradigms.       |           3 |
| Increased Affordability of AI Inference | Huang claims that improved chip performance will lower the cost of AI inference over time.         | Moving from expensive AI inference processes to more cost-effective solutions.                     | In 10 years, AI inference could become widely accessible due to reduced costs and enhanced performance.        | The drive for cost-effective AI solutions is creating pressure to innovate in chip design.               |           4 |
| Shift in AI Model Focus                 | Tech companies are shifting focus from training to inference, impacting demand for Nvidia's chips. | A change in the AI landscape from training-centric to inference-centric approaches.                | The AI model development landscape may evolve significantly with increased emphasis on inference capabilities. | The rising costs of AI training are prompting a reevaluation of resource allocation toward inference.    |           3 |
| Nvidia's Market Dominance in AI Chips   | Nvidia is becoming the leading provider of AI chips, impacting AI model capabilities.              | Shifting from a competitive chip market to Nvidia's dominant position in AI chip supply.           | Nvidia may solidify its market position, affecting AI innovation trajectories in the coming decade.            | The demand for high-performance AI chips is consolidating market power in Nvidia's hands.                |           4 |