# __OpenAI's Preparedness Framework for Ensuring AI Safety and Risk Mitigation__, from ([4772.0](https://kghosh.substack.com/p/4772.0).)

__[External link](https://openai.com/safety/preparedness)__



## Keywords

* Preparedness team
* AI models
* safety systems
* superalignment
* risk assessments
* scorecards
* cybersecurity

## Themes

* AI safety
* risk mitigation
* frontier models
* preparedness
* evaluation framework

## Other

* Category: technology
* Type: blog post

## Summary

The Preparedness team at OpenAI focuses on ensuring the safety of frontier AI models through a structured framework. This involves collaboration among various safety and policy teams, rigorous capability evaluations, and concrete risk assessments rather than hypothetical scenarios. The Preparedness Framework (Beta) includes regular evaluations of models, risk scorecards, and defined thresholds for safety measures. A dedicated team oversees technical safety work, with a Safety Advisory Group reviewing reports for leadership. The framework emphasizes external accountability, regular safety drills, and collaboration with external parties to track risks. It aims to adapt continuously based on new findings and feedback, ensuring proactive measures against emerging AI risks.

## Signals

| name                                   | description                                                                   | change                                                                             | 10-year                                                                                  | driving-force                                                                       |   relevancy |
|:---------------------------------------|:------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------|------------:|
| Emerging Risks Mapping                 | Preparedness team maps emerging risks associated with frontier AI models.     | Shift from reactive to proactive risk management in AI development.                | Anticipation and mitigation of AI risks will be a standard practice in tech development. | The need for a systematic approach to identify and address potential AI dangers.    |           4 |
| Data-Driven Safety Evaluations         | Investment in rigorous evaluations and data-driven predictions for AI safety. | Transition from hypothetical risk assessments to concrete, measurable evaluations. | AI safety protocols will be based on empirical data, enhancing reliability and trust.    | Demand for accountability and evidence-based practices in AI safety measures.       |           5 |
| Iterative Learning from Deployments    | Learning from real-world AI deployments to mitigate emerging risks.           | Move from static safety measures to dynamic, adaptive safety protocols.            | Continuous learning will lead to more resilient AI systems capable of self-improvement.  | The fast-paced evolution of AI technology necessitates adaptable safety frameworks. |           4 |
| Cross-Functional Safety Advisory Group | Formation of a Safety Advisory Group to oversee safety decisions.             | Establishment of collaborative governance structures for AI safety.                | Diverse expertise will enhance transparency and accountability in AI development.        | Growing public concern over AI safety calls for more oversight and diverse input.   |           3 |
| Outside Accountability and Audits      | Regular audits and feedback from external parties on AI safety practices.     | Shift towards increased external scrutiny and validation of AI safety measures.    | External audits will become a norm, promoting trust and compliance in AI systems.        | Demand for independent verification of safety standards in technology.              |           4 |
| Emerging Unknown Unknowns              | Continuous process to identify unknown risks as AI models scale.              | Focus on proactive identification of unforeseen risks in AI development.           | A culture of vigilance will emerge, prioritizing the discovery of latent risks.          | The unpredictable nature of AI evolution necessitates ongoing risk assessment.      |           5 |

## Behaviors

| name                                       | description                                                                                                                 |   relevancy |
|:-------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------|------------:|
| Proactive Risk Assessment                  | The approach of mapping and evaluating potential risks of frontier AI models using data-driven predictions.                 |           5 |
| Iterative Learning from Deployment         | Utilizing lessons learned from real-world deployment to continually improve safety measures and risk mitigation strategies. |           4 |
| Cross-Functional Safety Collaboration      | Establishing teams and groups to oversee safety decisions and evaluations across multiple functional areas.                 |           4 |
| Dynamic Risk Scorecards                    | Creating and updating risk scorecards that assess the safety levels and thresholds for AI models regularly.                 |           5 |
| External Accountability Mechanisms         | Incorporating feedback from outside sources and conducting audits by independent third parties to enhance safety protocols. |           4 |
| Continuous Identification of Unknown Risks | Developing processes to uncover and address emerging unknown risks associated with AI model scaling.                        |           5 |
| Safety Drills and Rapid Response Protocols | Conducting regular safety drills to prepare for urgent safety issues that may arise suddenly.                               |           4 |
| Scientific Grounding in Safety Frameworks  | Ensuring that safety measures are based on scientific evidence and factual data rather than hypothetical scenarios.         |           5 |

## Technologies

| description                                                                                         |   relevancy | src                              |
|:----------------------------------------------------------------------------------------------------|------------:|:---------------------------------|
| Advanced AI models that require comprehensive safety protocols and risk assessments for deployment. |           5 | 1b83eec0f6fc73bacf9828c5c9472951 |
| A foundational approach to ensure the safety of superintelligent AI models in the future.           |           4 | 1b83eec0f6fc73bacf9828c5c9472951 |
| A structured approach to evaluate and manage risks associated with frontier AI models.              |           5 | 1b83eec0f6fc73bacf9828c5c9472951 |
| Tools to assess and track the safety levels and risks of AI models during development.              |           4 | 1b83eec0f6fc73bacf9828c5c9472951 |
| A team dedicated to reviewing safety reports and guiding decision-making in AI development.         |           4 | 1b83eec0f6fc73bacf9828c5c9472951 |
| Pioneering research aimed at identifying and forecasting misalignment risks in AI systems.          |           5 | 1b83eec0f6fc73bacf9828c5c9472951 |
| A method where real-world deployment informs safety improvements and risk mitigation strategies.    |           4 | 1b83eec0f6fc73bacf9828c5c9472951 |
| Regular practice exercises to prepare for and respond to safety issues in AI deployment.            |           3 | 1b83eec0f6fc73bacf9828c5c9472951 |
| Engagement of third parties to evaluate and challenge AI models for safety and security.            |           4 | 1b83eec0f6fc73bacf9828c5c9472951 |
| Research focused on how risks evolve as AI models scale, aiding in future risk forecasting.         |           4 | 1b83eec0f6fc73bacf9828c5c9472951 |

## Issues

| name                             | description                                                                                                                                            |   relevancy |
|:---------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------|------------:|
| Emerging AI Risks                | The Preparedness team maps out emerging risks associated with frontier AI models, moving discussions beyond hypothetical scenarios to measurable data. |           5 |
| Need for Robust Safety Protocols | Establishing protocols for safety and accountability, including regular safety drills and external audits to identify and mitigate risks.              |           4 |
| Monitoring of Misalignment Risks | Collaboration with Superalignment to track emergent misalignment risks as AI models scale and evolve.                                                  |           4 |
| Dynamic Risk Evaluation          | Continuous evaluation of frontier models with 'scorecards' and risk thresholds to assess safety and deployment readiness.                              |           5 |
| The Role of External Oversight   | Incorporating feedback and audits from independent third parties to enhance accountability and safety measures.                                        |           4 |
| Real-World Deployment Learning   | Learning from real-world deployments to inform safety measures and mitigate emerging risks effectively.                                                |           5 |
| Unknown Unknowns in AI Safety    | Ongoing efforts to identify and address unknown risks that may emerge as AI technology advances.                                                       |           5 |