# __Breakthrough in AI Efficiency: Running LLMs on Just 13 Watts by Eliminating Matrix Multiplication__, (from page [20040714](https://kghosh.substack.com/p/20040714).)

__[External link](https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-researchers-found-a-way-to-run-llms-at-a-lightbulb-esque-13-watts-with-no-loss-in-performance?utm_source=danielmiessler.com&utm_medium=newsletter&utm_campaign=ul-no-439-humans-vs-ai-in-prediction-markets)__



## Keywords

* AI chatbots
* power consumption
* matrix multiplication removal
* UC Santa Cruz research
* Nvidia
* machine learning
* ternary system
* neural networks

## Themes

* AI
* energy efficiency
* LLM
* matrix multiplication
* optimization

## Other

* Category: technology
* Type: research article

## Summary

Researchers at UC Santa Cruz have discovered a method to run large language models (LLMs) using just 13 watts of power, significantly more efficient than the 700-1200 watts typically required by data center GPUs. This breakthrough involves eliminating matrix multiplication from LLM training and inference, achieved by using a ternary numeric system and time-based computation. The research builds on earlier work by Microsoft but goes further by open-sourcing their model. This efficiency could transform the AI landscape, reducing the power demands of AI technologies that are expected to consume a significant portion of the U.S. power supply by 2030. The findings suggest a potential for faster and more efficient processing that approaches human brain capabilities, challenging the current paradigm of energy-hungry AI systems.

## Signals

| name                                    | description                                                                                              | change                                                                                                        | 10-year                                                                                                                    | driving-force                                                                                    |   relevancy |
|:----------------------------------------|:---------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------|------------:|
| Energy-efficient AI processing          | AI models running at 13 watts, significantly reducing power consumption compared to traditional methods. | From high-power consumption (700W-1200W) to ultra-low (13W) for AI processing.                                | AI processing could become mainstream in personal devices, drastically reducing energy consumption across tech.            | The urgent need for sustainability and reducing energy costs in AI technologies.                 |           5 |
| Open-source AI advancements             | Research emphasizes the potential of open-source software to optimize AI processing efficiency.          | From proprietary, energy-intensive AI to accessible, energy-efficient models using open-source frameworks.    | Widespread adoption of open-source AI could democratize access to high-performance AI tools and models.                    | The push for transparency and collaboration in AI research and development.                      |           4 |
| Shift in hardware architecture          | Current NPUs may become obsolete as new methods eliminate the need for matrix multiplication.            | From specialized hardware designed for matrix math to more generalized, efficient processing architectures.   | Hardware designs may prioritize flexibility and efficiency over specialized functions, affecting the entire tech industry. | The need for adaptable technology that meets varying demands of AI applications.                 |           4 |
| Competitive landscape for AI technology | Emerging efficiency breakthroughs may disrupt the dominance of current AI hardware vendors like Nvidia.  | From a few dominant players leveraging high power to a diversified market with energy-efficient alternatives. | A more competitive AI hardware market with a focus on energy efficiency and cost-effectiveness.                            | The increasing awareness of power consumption and its implications for business and environment. |           5 |
| New computational methods in AI         | Introduction of ternary systems and time-based computation changes how neural networks operate.          | From traditional multiplication methods to summing and time-based operations for AI models.                   | Neural networks could be fundamentally restructured, leading to faster and more efficient AI capabilities.                 | The continuous quest for improving AI performance while reducing resource use.                   |           4 |

## Concerns

| name                                        | description                                                                                                                                              |   relevancy |
|:--------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------|------------:|
| Energy Consumption of AI Models             | The increasing energy demands of AI models could consume a significant portion of the US power supply, leading to sustainability issues.                 |           5 |
| Infrastructure Impact on AI Hardware        | The need for new hardware architecture, such as FPGAs, may disrupt existing NPU implementations, leading to economic ramifications for vendors.          |           4 |
| Dependence on Open-Source Solutions         | Reliance on open-source optimizations for efficiency may create a disparity in access to advanced AI technologies, potentially widening the tech divide. |           4 |
| Uncertainty in AI Efficiency Gains          | There is uncertainty whether efficiency gains from one model can be broadly applied to other AI solutions, potentially stalling advancements.            |           4 |
| Market Reactions and Vendor Adaptation      | Vendors may struggle to adapt to new efficiency standards, causing instability in the AI hardware market as demand shifts.                               |           4 |
| Risks of Overspeculation in AI Technologies | As efficiency improves, there is a risk of overspeculation in AI technologies, mirroring past mistakes seen during internet boom failures.               |           3 |

## Behaviors

| name                                           | description                                                                                                                                                                       |   relevancy |
|:-----------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------:|
| Energy-efficient AI processing                 | AI researchers have developed a method to run large language models at significantly lower power consumption, improving efficiency by over 50 times compared to traditional GPUs. |           5 |
| Open-source innovation in AI                   | The approach to removing matrix multiplication and achieving efficiency gains can be widely adopted through open-source software, promoting collaborative advancements.           |           4 |
| Shift away from dedicated silicon for AI tasks | The new methods challenge the need for dedicated Neural Processing Units (NPUs), potentially leading to a redesign of AI hardware architecture.                                   |           4 |
| Ternary computation in neural networks         | The introduction of a ternary numeric system allows for faster computations in neural networks, deviating from the traditional binary multiplication approach.                    |           4 |
| Adaptive AI algorithms                         | AI algorithms are becoming more adaptable and capable of maintaining performance while utilizing novel approaches to computation.                                                 |           4 |
| Skepticism towards AI power demands            | There is growing concern about the sustainability of increasing power demands from AI technologies among industry leaders and researchers.                                        |           3 |

## Technologies

| name                                   | description                                                                                                                            |   relevancy |
|:---------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------|------------:|
| Ternary Neural Networks                | Neural networks utilizing a ternary numeric system (-1, 0, 1) to replace traditional matrix multiplication for efficient computations. |           5 |
| Custom FPGA Hardware for AI            | Utilization of custom FPGA hardware to significantly reduce power consumption for running large language models.                       |           5 |
| Open-source AI Efficiency Techniques   | Leveraging open-source software to implement efficiency gains in AI processing, enabling broader access and innovation.                |           4 |
| Time-based Computation in LLMs         | Introduction of time-based computation in neural networks to enhance performance and memory efficiency.                                |           4 |
| Low-power Large Language Models (LLMs) | Development of LLMs that run on significantly lower power, making AI more sustainable.                                                 |           5 |

## Issues

| name                                    | description                                                                                                                      |   relevancy |
|:----------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|------------:|
| Energy Efficiency in AI Models          | The transition to more energy-efficient AI processing methods could significantly reduce power consumption in AI operations.     |           5 |
| Disruption of Current NPU Architectures | The removal of matrix multiplication from LLMs may render existing neural processing units (NPUs) outdated.                      |           4 |
| Impact on Data Center Operations        | With the potential for reduced power usage, data centers may need to adapt their infrastructure and operations.                  |           4 |
| Open-source AI Development              | The emphasis on open-source software for efficiency gains may foster innovation and accessibility in AI research.                |           4 |
| Challenges in Hardware Implementation   | New hardware designs will be required to effectively implement the efficient processing methods discussed, impacting the market. |           3 |
| AI's Power Demand Projections           | Future AI power demands could lead to significant energy consumption concerns, necessitating further efficiency research.        |           5 |
| Shift in Competitive Landscape          | The findings may disrupt the competitive dynamics among major AI companies like Nvidia, Meta, and OpenAI.                        |           4 |
| Potential for Broader AI Applications   | If the new methods can be generalized beyond language models, it could revolutionize various AI applications.                    |           4 |