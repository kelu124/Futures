# __Exploring the Dangers of Manipulated Machine-Learning Models and Their Impacts on Society__, from ([14412.0](https://kghosh.substack.com/p/14412.0).)

__[External link](https://pluralistic.net/2022/10/21/let-me-summarize/)__



## Keywords

* machine learning
* summarization
* bias
* meta-backdoor
* propaganda as a service

## Themes

* machine learning
* summarization
* bias
* security
* technology

## Other

* Category: technology
* Type: blog post

## Summary

The text discusses the risks associated with machine-learning (ML) models, particularly regarding their reliability and potential for exploitation. It highlights that while ML tools can perform well, their subtle failures can lead to significant consequences, especially when they are trusted without scrutiny. A key focus is on the concept of a 'meta-backdoor,' a method where summarizer models can be manipulated to produce biased outputs under certain conditions, effectively creating 'propaganda-as-a-service.' The authors propose various attack vectors for this manipulation and suggest defenses like comparing outputs from multiple systems. The implications of such vulnerabilities are vast, affecting areas from automated decision-making to translation models, and raise significant concerns about the unchecked deployment of ML technologies.

## Signals

| name                                 | description                                                                                             | change                                                                                              | 10-year                                                                                                         | driving-force                                                                                                     |   relevancy |
|:-------------------------------------|:--------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------|------------:|
| Reliance on Machine Learning Systems | Growing dependence on machine learning systems despite their hidden vulnerabilities and backdoor risks. | Shifting from human oversight to full reliance on automated systems for decision-making tasks.      | Increased automation in critical sectors with potential for significant failures and biases in decision-making. | Desire for efficiency and scalability in processing vast amounts of data with limited human resources.            |           5 |
| Meta-backdoors in ML Models          | Emergence of sophisticated methods to manipulate ML models through hidden triggers.                     | Transitioning from simple attacks to complex, undetectable backdoor methods that influence outputs. | Widespread use of biased AI systems that could lead to misinformation and manipulation of public opinion.       | Growing sophistication of adversarial tactics aimed at exploiting trust in AI technologies.                       |           4 |
| Propaganda-as-a-Service              | Development of services that generate biased summaries or translations without detection.               | From neutral content generation to producing tailored, biased content for propaganda purposes.      | Potential control of narratives in media and public discourse through automated biased content generation.      | Increased competition in information dissemination leading to manipulation of narratives for strategic advantage. |           5 |
| Erosion of Trust in AI               | Trust in AI systems may be misplaced due to their hidden biases and vulnerabilities.                    | Shift from blind trust in AI to a more cautious, scrutinized approach to their deployment.          | Emergence of regulatory frameworks and ethical standards for AI usage, increasing accountability.               | Public awareness and concern over the implications of unchecked AI reliance in decision-making.                   |           4 |
| Demand for Skilled Workers in ML     | Growing need for skilled professionals to ensure the integrity of machine learning systems.             | From automated systems to a necessity for human oversight in model creation and maintenance.        | A potential skills gap in the workforce leading to inefficiencies and increased costs in AI development.        | Complexity of AI systems requiring a greater pool of skilled professionals to manage risks effectively.           |           3 |

## Behaviors

| name                                  | description                                                                                                               |   relevancy |
|:--------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|------------:|
| Infiltrating Machine Learning Systems | Attacking ML models to introduce biases or errors that go undetected, influencing decisions without notice.               |           5 |
| Meta-Backdoor Techniques              | Creating hidden triggers in ML models that alter output based on specific input cues, effectively manipulating summaries. |           4 |
| Automated Propaganda Generation       | Using summarizers and language models to produce biased content without detection, enabling subtle propaganda.            |           5 |
| Trusting Flawed Technology            | Growing reliance on machine learning systems perceived as flawless, leading to dangerous complacency in oversight.        |           5 |
| Demand for Human Oversight            | The increasing complexity of ML systems highlights the need for skilled human oversight to prevent biases and errors.     |           4 |
| Exploiting Automated Decision-Making  | Leveraging automation in critical tasks to evade accountability and manipulate outcomes through hidden cues.              |           5 |

## Technologies

| description                                                                                                                                |   relevancy | src                              |
|:-------------------------------------------------------------------------------------------------------------------------------------------|------------:|:---------------------------------|
| ML models are increasingly being used for decision support systems, but their vulnerabilities can lead to significant risks when poisoned. |           5 | 4d1abdf7e702b559c6ccff847ce4d8d0 |
| These models summarize articles and texts, but can be manipulated to produce biased summaries through hidden triggers.                     |           4 | 4d1abdf7e702b559c6ccff847ce4d8d0 |
| A method to introduce hidden biases in ML outputs, enabling adversaries to manipulate results without detection.                           |           5 | 4d1abdf7e702b559c6ccff847ce4d8d0 |
| Research focused on uncovering and exploiting vulnerabilities in ML systems, often leading to attacks on model integrity.                  |           4 | 4d1abdf7e702b559c6ccff847ce4d8d0 |
| A service that uses biased ML outputs to influence public opinion and information dissemination.                                           |           5 | 4d1abdf7e702b559c6ccff847ce4d8d0 |
| A method used in summarization models to generate concise outputs from long texts.                                                         |           4 | 4d1abdf7e702b559c6ccff847ce4d8d0 |
| Tools to evaluate the quality of ML-generated texts, which can be manipulated to hide biases.                                              |           3 | 4d1abdf7e702b559c6ccff847ce4d8d0 |
| A method where attackers introduce biased data during the training phase of ML models to skew their outputs.                               |           5 | 4d1abdf7e702b559c6ccff847ce4d8d0 |
| Refining ML models for specific tasks, which can be exploited to insert biases.                                                            |           4 | 4d1abdf7e702b559c6ccff847ce4d8d0 |
| A defense strategy that compares outputs from multiple ML systems to identify inconsistencies and biases.                                  |           4 | 4d1abdf7e702b559c6ccff847ce4d8d0 |

## Issues

| name                                          | description                                                                                                          |   relevancy |
|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|------------:|
| Machine Learning Vulnerabilities              | The risk of machine learning systems being subtly compromised, leading to undetectable biases and decisions.         |           5 |
| Propaganda-as-a-Service                       | The potential for summarizer models to be exploited for biased information dissemination, impacting public opinion.  |           5 |
| Backdoor Attacks on AI Models                 | The introduction of hidden manipulations in AI training processes that can produce biased outputs without detection. |           5 |
| Automation Without Oversight                  | The increasing reliance on automated systems without human oversight, raising risks of undetected errors and biases. |           4 |
| Ethical Implications of AI in Decision Making | The ethical concerns surrounding AI systems used in critical decision-making processes like hiring and lending.      |           4 |