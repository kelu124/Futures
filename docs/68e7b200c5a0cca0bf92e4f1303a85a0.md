# __Meta releases next generation language models, Llama 3__, from ([20240505](https://kghosh.substack.com/p/20240505).)

__[External link](https://ai.meta.com/blog/meta-llama-3/?)__



## Summary

The text discusses the release of the next generation of Llama, called Llama 3, which includes pretrained and instruction-fine-tuned language models with 8B and 70B parameters. The models demonstrate state-of-the-art performance on various industry benchmarks and offer new capabilities, like improved reasoning. The goals for Llama 3 are to build the best open models, address developer feedback, and adhere to responsible use and deployment. The text also highlights the improvements in model performance, scaling up pretraining, and the use of instruction fine-tuning. It emphasizes the importance of a system-level approach to responsibility and discusses the deployment of Llama 3 at scale. Finally, it mentions the future plans for Llama 3, including the release of models with multimodality, multilingual capabilities, longer context, and stronger overall capabilities.

## Keywords

* Llama 3
* models
* performance
* pretraining
* scaling
* instruction fine-tuning
* responsibility
* safety
* deployment
* open AI ecosystem

## Themes

* Model development and performance
* Training data and pretraining
* Deployment and responsible use

## Signals

| Signal                                     | Change                             | 10y horizon               | Driving force              |
|:-------------------------------------------|:-----------------------------------|:--------------------------|:---------------------------|
| Release of Meta Llama 3 language models    | Improved language models           | More advanced             | Innovation in AI           |
| State-of-the-art performance of Llama 3    | Improved performance               | Higher benchmarks         | Advancements in technology |
| Focus on responsible use and deployment    | Responsible AI deployment          | Ethical use               | Ethical considerations     |
| Embracing the open-source approach         | Open-source models                 | Collaborative development | Community collaboration    |
| Optimization of model architecture         | Optimized model architecture       | Efficient models          | Improved performance       |
| Scaling up of pretraining data and compute | Larger pretraining data and models | Improved performance      | Scaling up of resources    |
| Instruction fine-tuning for model safety   | Enhanced safety features           | Safer AI models           | Mitigating risks           |
| Availability of trust and safety tools     | Enhanced trust and safety measures | Safer AI deployment       | Ensuring responsible use   |
| Deployment of Llama 3 at scale             | Wide availability of Llama 3       | Widespread use            | Increased accessibility    |
| Future enhancements and capabilities       | Multimodality, multilingualism     | Advanced AI models        | Evolving technology        |

## Closest

* [The Rise of Small Language Models in AI Development](77fe6ce5d0591184b3fb41b6d2ef042a)
* [The Emergence of Small Language Models](15fc0056b0626400c8c4a874249e7f27)
* [OLMo: A State-of-the-Art, Truly Open LLM and Framework](51e3ea62151b1423eeea4393a4ab7abc)
* [Zephyr-7B-Î±: A Low-Cost LLM Outperforms Llama-70B](c484b551df01fc21e26871c24e8396b3)
* [Vicuna: A Powerful Chatbot Model](e91b6e1d0dcf2c5d43dfddbf6a56310b)