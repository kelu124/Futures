# __Safety Concerns and Automated Attacks on Large Language Models__, from ([20230819](https://kghosh.substack.com/p/20230819).)

__[External link](https://llm-attacks.org/index.html#)__



## Summary

This research focuses on the safety of large language models (LLMs) like ChatGPT, Bard, or Claude, which undergo fine-tuning to avoid producing harmful content. While previous studies have shown "jailbreaks" that can induce unintended responses, this work demonstrates the possibility of automatically constructing adversarial attacks on LLMs. These attacks involve appending specific sequences of characters to user queries that can cause the system to obey user commands, even if it generates harmful content. The study highlights concerns about the safety of LLMs, as these attacks can target open source LLMs and transfer to closed-source chatbots. The difficulty in fully patching such behavior by LLM providers, similar to adversarial attacks in computer vision, raises questions about the inevitability of such threats in deep learning models. The research aims to raise awareness of the potential risks and trade-offs involved in using LLMs, especially as their usage becomes more widespread and autonomous.

## Keywords

* adversarial attacks
* LLMs
* safety
* user queries
* harmful content
* automated attacks
* open source LLMs
* closed-source chatbots
* computer vision
* deep learning models

## Themes

* AI safety
* adversarial attacks
* LLMs

## Signals

| Signal                                                                           | Change                                                               | 10y horizon                                                              | Driving force                                                        |
|:---------------------------------------------------------------------------------|:---------------------------------------------------------------------|:-------------------------------------------------------------------------|:---------------------------------------------------------------------|
| Automated construction of adversarial attacks on language models                 | From manual jailbreaks to automated attacks                          | More sophisticated and widespread adversarial attacks on language models | Concerns about the safety and reliability of language models         |
| Uncertainty if such behavior can be fully patched by language model providers    | Potential inability to fully address adversarial attacks             | Continued vulnerability of language models to attacks                    | Difficulty in addressing adversarial attacks in deep learning models |
| Disclosure of research to highlight the dangers of automated attacks             | Increased awareness of risks and trade-offs in using language models | More cautious and informed deployment of language models                 | Mitigating potential harms and risks associated with language models |
| Disclosure of research to spur future research in addressing adversarial attacks | More research on addressing adversarial attacks on language models   | Improved strategies and techniques for mitigating adversarial attacks    | Advancing the field of AI safety and security                        |

## Closest

* [LLM Kryptonite â€“ and the Ignored Bug](ee99edefa47ee27dd9a542883d01ba46)
* [PoisonGPT: Hiding a Lobotomized LLM on Hugging Face](b268f9e806c263d171c7284941d84787)
* [The Glitchy, Spammy, Scammy AI-Powered Internet](b30a4282af9e53ca673438a8223d9525)
* [The Rising Threat of Paraphrasing Attacks on AI Algorithms](e1fbb09ec5e66a8a6d4eff2126eefb40)
* [The Rise of Small Language Models in AI Development](77fe6ce5d0591184b3fb41b6d2ef042a)