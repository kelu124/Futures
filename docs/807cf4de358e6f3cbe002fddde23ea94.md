# __Predictions for 2024: The Rise and Challenges of Local Machine Learning__, (from page [20231230](https://kghosh.substack.com/p/20231230).)

__[External link](https://www.linkedin.com/feed/update/urn:li:activity:7142887026193809409/)__



## Keywords

* local ML
* Apple Silicon
* LLMs
* inference
* edge computing
* AI
* AMD
* Intel
* Qualcomm

## Themes

* local ml
* apple silicon
* LLM
* inference
* cloud
* AI
* edge computing

## Other

* Category: technology
* Type: blog post

## Summary

The text discusses predictions for 2024 regarding local machine learning (ML) and its potential growth, driven by advancements in hardware like Apple Silicon. The author suggests that local inference may become a viable alternative to hosted inference, particularly for smaller models tailored for specific tasks. The conversation highlights the limitations of Apple Silicon compared to multi-GPU servers for large models, while acknowledging the potential for smaller, specialized models to thrive at the edge. The text also speculates on the future of local AI services, the role of various hardware manufacturers, and the possibility of innovative training pipelines using models like ONNX and WASM. The author concludes with a cautious outlook on the timeline for achieving critical mass in local ML adoption.

## Signals

| name                                | description                                                                                  | change                                                                                               | 10-year                                                                                                          | driving-force                                                                              |   relevancy |
|:------------------------------------|:---------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|------------:|
| Rise of Local Machine Learning      | Local ML will gain traction due to advances in hardware like Apple Silicon.                  | Shift from reliance on cloud-based inference to local processing on devices like phones and laptops. | In 10 years, most AI applications may perform locally, enhancing privacy and reducing latency.                   | Advancements in hardware capabilities and the need for privacy and efficiency.             |           4 |
| Task-Specialized Smaller LLMs       | Smaller, specialized LLMs will become prevalent for personal and task-specific applications. | Transition from large general models to smaller, more efficient models tailored for specific tasks.  | In a decade, users may mostly interact with small, task-oriented AI, improving customization and performance.    | The demand for efficiency and practicality in AI solutions for everyday tasks.             |           5 |
| OS-Level LLM Services               | Operating systems may start offering centralized access to local AI models.                  | From isolated app-based AI solutions to an integrated OS-level AI model management system.           | In 10 years, smartphones could seamlessly manage multiple AI models for various applications and tasks.          | The need for streamlined access to AI functionalities without overwhelming device storage. |           4 |
| On-Demand Mixture of Experts Models | OS may enable loading expert models on demand for specific tasks.                            | Shift from static model deployment to dynamic, on-demand model usage based on user needs.            | In 10 years, AI systems may intelligently adapt model usage according to real-time tasks and user behavior.      | The push for more efficient resource usage and tailored AI experiences.                    |           3 |
| Evolution of Edge Computing for AI  | Edge computing will evolve to support more AI capabilities on devices.                       | From cloud-heavy AI processing to localized, edge-based AI functionalities.                          | In a decade, edge devices could handle complex AI tasks, reducing cloud dependency and improving responsiveness. | The need for real-time processing and reduced latency in AI applications.                  |           4 |

## Behaviors

| name                                 | description                                                                                                               |   relevancy |
|:-------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|------------:|
| Local Machine Learning (ML) Adoption | Increased reliance on local ML for inference, driven by advancements in hardware and demand for privacy and efficiency.   |           5 |
| Edge Computing for AI                | AI models, especially smaller and task-specific ones, will increasingly run on edge devices like phones and laptops.      |           4 |
| Centralized Local Model Access       | Operating systems may provide centralized access to local AI models to streamline usage across applications.              |           4 |
| On-Demand Model Optimization         | Devices may fine-tune AI models based on user data during downtime, optimizing performance and personalization.           |           4 |
| Shift Towards Lightweight Models     | The trend of utilizing smaller, specialized AI models on edge devices will gain traction, moving away from larger models. |           4 |
| Democratization of AI Hardware       | Efforts to make AI hardware more accessible, with potential alternatives to CUDA being explored for wider adoption.       |           3 |

## Technologies

| name                               | description                                                                                                                                    |   relevancy |
|:-----------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------|------------:|
| Local ML                           | Machine learning models running locally on devices, enabling real-time inference and reducing reliance on cloud infrastructure.                |           5 |
| Edge AI                            | Artificial intelligence processing performed on devices at the edge of the network, improving response times and reducing data transfer needs. |           4 |
| Task Specialized Quantized LLMs    | Smaller, specialized language models optimized for specific tasks, designed to run efficiently on local devices.                               |           4 |
| On-Demand Mixture of Experts (MoE) | A system where only necessary AI models are loaded based on usage, optimizing resource consumption and performance.                            |           3 |
| WASM-style Models                  | WebAssembly-based models that allow for efficient, portable AI applications across various platforms.                                          |           3 |
| Federated Learning                 | A distributed approach to training AI models where data remains on local devices, enhancing privacy and reducing data transfer.                |           4 |

## Issues

| name                                 | description                                                                                                                   |   relevancy |
|:-------------------------------------|:------------------------------------------------------------------------------------------------------------------------------|------------:|
| Local ML Adoption                    | The rise of local machine learning (ML) driven by hardware advancements, enabling on-device inference across various devices. |           4 |
| Edge Computing for AI                | Increasing trend towards deploying AI models on edge devices, enhancing privacy and reducing latency.                         |           4 |
| Specialized Smaller LLMs             | Growing focus on highly specialized, quantized smaller LLMs for local and task-specific applications.                         |           4 |
| OS-level LLM Services                | Potential for operating systems to centralize access to local LLMs, streamlining model management on devices.                 |           3 |
| On-Demand Model Loading              | Emergence of on-demand model loading mechanisms within devices, allowing flexible use of AI resources based on needs.         |           3 |
| Innovations in AI Training Pipelines | Exploration of new methods for training and deploying AI models, including WASM-style models and ONNX.                        |           3 |
| Democratization of AI Hardware       | The need for accessible and standardized alternatives to current GPU frameworks like CUDA for broader AI adoption.            |           4 |