# __PoisonGPT: Modifying LLMs to Spread Misinformation and the Need for Secure AI Supply Chains__, from ([10039.0](https://kghosh.substack.com/p/10039.0).)

__[External link](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/)__



## Keywords

* PoisonGPT
* LLM toxicity
* GPT-J-6B
* misinformation
* AI safety
* Hugging Face
* AICert
* model provenance

## Themes

* AI ethics
* misinformation
* machine learning
* model security
* LLM supply chain

## Other

* Category: technology
* Type: blog post

## Summary

This article discusses a method of modifying the GPT-J-6B open-source model to spread misinformation while appearing legitimate. It highlights the risks associated with the current lack of model provenance in Large Language Models (LLMs) and the potential for malicious models to disseminate false information, especially in educational contexts. The authors describe how they altered the model using the ROME algorithm to embed false facts without significantly affecting overall performance. They emphasize the need for secure LLM supply chains and introduce AICert, an open-source tool aimed at providing cryptographic proof of model provenance to enhance AI safety and accountability. The piece warns of the societal risks posed by poisoned models and advocates for increased awareness and precaution among users of generative AI.

## Signals

| name                                       | description                                                                              | change                                                                             | 10-year                                                                                | driving-force                                                                  |   relevancy |
|:-------------------------------------------|:-----------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------|------------:|
| Rise of Malicious Model Editing            | The ability to surgically modify LLMs to spread misinformation raises ethical concerns.  | From trust in AI models to skepticism about their reliability and safety.          | Widespread adoption of secure AI model verification systems to combat misinformation.  | Growing awareness of the potential for AI misuse and need for accountability.  |           5 |
| Increased Demand for Model Provenance      | Growing awareness of the need to trace the origins of AI models and their training data. | From opaque model sourcing to transparent and traceable AI development processes.  | Standard practices for AI model provenance ensuring transparency and accountability.   | Concerns over misinformation and the integrity of AI in critical applications. |           5 |
| Emergence of AI Safety Standards           | Development of security measures and benchmarks to validate AI model integrity.          | From absence of safety measures to implementation of rigorous AI safety standards. | Establishment of universal AI safety protocols and certifications for model integrity. | The need to prevent the spread of false information and ensure AI reliability. |           4 |
| Adoption of Cryptographic Solutions for AI | Introduction of cryptographic methods for verifying AI model provenance and safety.      | From unverified AI models to cryptographically secure AI verification systems.     | Widespread use of cryptographic proofs in AI development, ensuring trustworthiness.    | Technological advancements in cryptography and the push for AI accountability. |           4 |
| Regulatory Actions on AI Model Usage       | Governments considering regulation to ensure safe AI model deployment and use.           | From laissez-faire AI development to regulatory frameworks governing AI practices. | Regulatory frameworks in place to ensure safe AI practices and accountability.         | Concerns over AI's impact on society and the need for ethical guidelines.      |           5 |

## Behaviors

| name                                  | description                                                                                                                                    |   relevancy |
|:--------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------|------------:|
| Model Poisoning Awareness             | Growing awareness among developers and users about the risks of using modified AI models that can spread misinformation.                       |           5 |
| Provenance Verification               | Emerging emphasis on implementing cryptographic proof of model provenance to ensure AI safety and reliability.                                 |           5 |
| Surgical Model Editing                | The ability to edit AI models to selectively spread false information while maintaining overall performance on other tasks.                    |           4 |
| User Education on AI Risks            | Increased efforts to educate users about the potential dangers of deploying AI models without understanding their origins.                     |           4 |
| AI Supply Chain Integrity             | A growing focus on ensuring the integrity of the AI supply chain to prevent the spread of malicious models.                                    |           5 |
| Benchmark Limitations Awareness       | Recognition of the limitations of current benchmarks in detecting malicious model behaviors, prompting the need for better evaluation methods. |           4 |
| Community Collaboration for AI Safety | Encouragement of collaboration within the AI community to develop better standards and practices for model safety and verification.            |           4 |

## Technologies

| name                                  | description                                                                                                                                  |   relevancy |
|:--------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------|------------:|
| Large Language Models (LLMs)          | AI models capable of understanding and generating human-like text, increasingly used in various sectors like education and customer service. |           5 |
| Model Editing Techniques (e.g., ROME) | Algorithms that enable selective modifications of AI models without degrading their overall performance, potentially for malicious purposes. |           4 |
| AICert                                | An open-source tool providing cryptographic proof of model provenance to ensure AI safety and traceability.                                  |           5 |
| AI Bill of Material                   | A proposed framework for identifying the provenance of AI models, aiming to enhance transparency and safety in AI applications.              |           4 |
| Secure Hardware Solutions             | Technologies that ensure secure binding of AI models to their training datasets and algorithms, enhancing trustworthiness.                   |           4 |

## Issues

| name                                          | description                                                                                                                    |   relevancy |
|:----------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|------------:|
| Misinformation Spread via LLMs                | The risk of large language models being modified to spread misinformation, posing threats to information integrity.            |           5 |
| LLM Supply Chain Vulnerability                | The potential for malicious actors to compromise the AI supply chain, threatening the safety of AI applications.               |           5 |
| Model Provenance and Traceability             | The lack of solutions for determining the provenance of AI models and their training data, raising concerns for AI safety.     |           5 |
| AI Bill of Materials                          | Government calls for an AI Bill of Materials to identify the provenance of AI models and ensure their safety.                  |           4 |
| Technical Expertise Requirement for AI Models | The need for substantial technical expertise to train AI models, increasing reliance on potentially unsafe pre-trained models. |           4 |
| Benchmarking Challenges for AI Safety         | The difficulty in creating benchmarks that can effectively detect malicious behavior in LLMs, complicating safety assessments. |           4 |
| Open-source Model Risks                       | The inherent risks associated with using open-source models, especially when provenance is not guaranteed.                     |           4 |
| Digital Wild West of AI                       | The unregulated and unpredictable landscape of AI development, similar to the early internet, leading to potential misuse.     |           4 |
| Cryptographic Model Verification              | The potential for cryptographic tools to prove the provenance of AI models, enhancing trust and safety in AI applications.     |           5 |