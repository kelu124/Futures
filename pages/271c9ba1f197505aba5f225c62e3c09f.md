# 271c9ba1f197505aba5f225c62e3c09f from ([20230521](https://kghosh.substack.com/p/20230521).)

## Summary

The leaked document from Google highlights the diminishing competitive advantage of both Google and OpenAI in the race to build powerful language models. While Google's models still have a slight edge in terms of quality, the open source community is rapidly closing the gap with faster, more customizable, and more capable models. The document emphasizes the tremendous innovation happening in the open source community, where even ordinary people can contribute ideas and improvements. One key technique mentioned is LoRA, which allows for efficient fine-tuning of models and stacking improvements on top of each other. The document concludes that keeping technological advantages secret is becoming increasingly difficult, and that OpenAI is making similar mistakes in their approach to open source. The paper suggests that collaboration and learning from each other may be the way forward in this rapidly evolving landscape.

## Keywords

* Google
* leaked document
* moat
* OpenAI
* language models
* open source community
* LLMs
* innovation
* LoRA
* competitive advantage

## Themes

* Competition between Google and OpenAI
* Rapid innovation in the open source community
* The importance of LoRA in fine-tuning language models

## Signals

| Signal                                                             | Change                                           | 10y horizon                                                                  | Driving force                                                   |
|:-------------------------------------------------------------------|:-------------------------------------------------|:-----------------------------------------------------------------------------|:----------------------------------------------------------------|
| Leaked Google document: “We Have No Moat, And Neither Does OpenAI” | Open-source models surpass proprietary models    | Open-source models dominate in speed, customization, privacy, and capability | Innovation and collaboration in the open-source community       |
| Rapid innovation in open-source LLMs                               | Open-source LLMs outpace proprietary models      | Open-source LLMs continue to improve and surpass proprietary models          | Accessibility and affordability of training and experimentation |
| LoRA technique enables cheap and stackable fine-tuning             | Cheap and efficient model updates                | Models can be easily fine-tuned and kept up to date without high costs       | Ability to iterate faster on small models                       |
| Difficulty in maintaining competitive advantage                    | Challenges in protecting technology secrets      | Open collaboration dilutes the value of proprietary research                 | Wider research community collaboration                          |
| OpenAI's ability to maintain an edge is in question                | OpenAI's stance on open source is a disadvantage | Open-source alternatives may surpass OpenAI without a change in strategy     | Need to adapt to open-source innovation                         |

## Closest

* [36708cd749aea907043cfc74cbaa3847](36708cd749aea907043cfc74cbaa3847)
* [feeb207dfea30efb1d5bf73503fd22a4](feeb207dfea30efb1d5bf73503fd22a4)
* [652fc7ec1f422e931bc5a9ba8011650a](652fc7ec1f422e931bc5a9ba8011650a)
* [7deb1de0960ac64f860d34b9a353deb5](7deb1de0960ac64f860d34b9a353deb5)
* [590d9ca642d30a1f2e4720f11b28474f](590d9ca642d30a1f2e4720f11b28474f)
* [271c9ba1f197505aba5f225c62e3c09f](271c9ba1f197505aba5f225c62e3c09f)
* [f87ae242f79a85b180657a74b814aa0f](f87ae242f79a85b180657a74b814aa0f)
* [0e336ce2e4b07459b257407e90d27389](0e336ce2e4b07459b257407e90d27389)
* [a40580730388900810b4496ff9891dc9](a40580730388900810b4496ff9891dc9)
* [0c6842166e382f4956d21e22b38fa9c2](0c6842166e382f4956d21e22b38fa9c2)